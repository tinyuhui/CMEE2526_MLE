{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0182cf96-7230-47aa-9173-5fe7ba63b2d9",
   "metadata": {},
   "source": [
    "# MLE Notebook 1: Probability review\n",
    "### 0. Contents\n",
    "Axioms. Common discrete and continuous random variables, probability mass and density functions, cumulative functions.\n",
    "\n",
    "Expecation, variance, statistical moments, moment generating functions. \n",
    "\n",
    "Central limit theorem, Weak law of large numbers. \n",
    "\n",
    "#### 0.1 Suggested readings\n",
    "1) Millar, *Maximum Likelihood Estimation and Inference*\n",
    "   \n",
    "2) Crawley, *The R Book*\n",
    "\n",
    "3) Hogg & Tanis, *Probability and Statistical Inference*\n",
    "\n",
    "#### 0.2 Acknowledgements\n",
    "This series of five notebooks are compiled by Tin-Yu J Hui based on existing materials. Special thanks to Dan Reuman who used to teach this module at Silwood some time ago. Some examples were extracted from Mick Crawley's GLM course, where I had the pleasure to attend, both as a student and teaching assistant. Any errors that remain are, of course, my sole responsibility. \n",
    "\n",
    "### 1. The three axioms of probability\n",
    "These axioms are the building blocks of modern theories of probability and statistics. \n",
    "\n",
    "1) For any event $A$ in a sample space $S$, $Pr(A)\\geqslant 0$\n",
    "\n",
    "2) $Pr(S)=1$\n",
    "   \n",
    "3) For disjoint events $A_1, A_2, A_3, ...$, then $Pr(A_1\\cup A_2\\cup A_3\\cup ...)=Pr(A_1)+Pr(A_2)+Pr(A_3)+...$\n",
    "\n",
    "We assign a probability measure $Pr(A)$ to an event $A$. The first axiom states that probabiliy is always non-negative. The smallest probability is zero (i.e. impossible). The second axiom states that the probability of the whole sample space is one. The sample space $S$ contains all possible outcomes for the given random experiment, and something must happen. This also specifies the upper bound for a probability. For the third axiom, the probability of the union of disjoint (i.e. non-overlapping) events equals the sum of their individual probabilities. Think of a Venn diagram. \n",
    "\n",
    "### 2. Random variables\n",
    "A random variable (r.v.) is a variable who takes on its value by chance. A r.v. can take on a set of possible values, each with an associated probability. To fully characterise a r.v. we need to know 1) all its possible outcomes, which form the domain or support of the r.v., and 2) the probability of getting each outcome. \n",
    "\n",
    "Example: Let $X$ be the outcome from a coin toss. Clearly $X$ is random. There can only be two possible outcomes: head or tail. If the coin is fair then $Pr(X=head)=Pr(X=tail)=0.5$. These two statements jointly characterise the r.v. $X$. \n",
    "\n",
    "#### 2.1 Discrete random variables\n",
    "Some r.v. take a discrete collection of values. We call them discrete r.v.. An example of a discrete r.v. is the outcome from rolling a fair die. \n",
    "\n",
    "A probability *mass* function (pmf) for a discrete r.v. $X$ is a function that describes the relative probability that $X$ takes each of its possible values. In most textbooks, the pmf is written as $f(x)$ or $f_X(x)$. See #2.3 for more notations. \n",
    "\n",
    "Below are some common discrete r.v..\n",
    "\n",
    "##### 2.1.1 Bernoulli random variable\n",
    "A Bernoulli r.v. is the simplest r.v. with two outcomes: success (1) or failure (0). It has one parameter $p$, the probability of success, which is bounded between 0 and 1. If $X\\sim Bernoulli(p)$ then it is obvious that $Pr(X=1)=p$ and $Pr(X=0)=1-p$. While these two statements technically summarise the probabilities, the pmf has an alternative expression: $f_X(x)=p^x(1-p)^{1-x}$. \n",
    "\n",
    "Note that $f_X(x)=0$ elsewhere (outside of the support), but this is often too trivial to be mentioned. \n",
    "\n",
    "##### 2.1.2 Binomial random variable\n",
    "A binomial r.v. is the sum of $n$ independent and identically distributed (i.i.d.) Bernoulli r.v. hence it takes values on $\\{0, 1, 2, ..., n\\}$. It is a two-parameter r.v.: $p$ the probability of success, inherited from the Bernoulli, and $n$ the number of i.i.d. Bernoulli trials. If $X\\sim binomial(n, p)$ then its pmf is\n",
    "$$f_X(x)=C^n_{x}p^x(1-p)^{n-x}$$\n",
    "where $C^n_{x}$ is the number of combinations when we choose $x$ objects from $n$. Order of selection does not matter here. Note that\n",
    "$$C^n_{x}=\\frac{n!}{x!(n-c)!}$$\n",
    "or you can use the <code>choose()</code> function in R. \n",
    "\n",
    "##### 2.1.3 Poisson random variable\n",
    "A Poisson r.v. models the number of events occurring in a fixed interval of time. Since it is a count variable, its possible values are all non-negative integers $\\{0, 1, 2, 3, ...\\}$. While there are infinitely many possible outcomes it is still regarded as a discrete r.v.. \n",
    "\n",
    "Poisson has one parameter which is the rate of occurrance $\\lambda>0$. If $X\\sim Poisson(\\lambda)$ then\n",
    "$$f_X(x)=\\frac{\\lambda^{x}e^{-\\lambda}}{x!}$$\n",
    "\n",
    "Note: If $X\\sim binomial(n, p)$ has a large $n$ and small $p$, and that $np$ is also reasonably small, then $X$ can be approximated by Poisson with $\\lambda=np$. That is, the number of rare events can be modelled by Poisson. \n",
    "\n",
    "#### 2.2 Continuous random variables\n",
    "Continuous r.v., unlike their discrete counterpart, take a whole range of real-number values (think of tomorrow's temperature or allele frequencies). To accommodate continuous r.v.., a probability *density* function (pdf) is in place to describe the relative probability that the r.v. takes each value in the range of possible values. \n",
    "\n",
    "Recall: The range of possible outcomes within non-zero probability is called the *support* of a r.v.. \n",
    "\n",
    "##### 2.2.1 Uniform random variable\n",
    "A uniform r.v. is a continous r.v. with two parameters $a$ and $b$, which are the lower and upper bounds. If $X\\sim U(a,b)$ then\n",
    "$$f_X(x)=1/(b-a)$$\n",
    "which looks like a horizontal line from $a$ to $b$. \n",
    "\n",
    "##### 2.2.2 Exponential random variable\n",
    "An Exponential r.v. models the time between two successive events (remember Poisson r.v.?). Since it is a measure of time, it is continuous, with support $[0, \\infty)$ (inclusive of 0, but always smaller than infinity). It shares the same  rate parameter $\\lambda$ with Poisson. If $X\\sim Exponential(\\lambda)$ then\n",
    "$$f_X(x)=\\lambda e^{-\\lambda x}$$\n",
    "\n",
    "##### 2.2.3 Normal random variable\n",
    "Later we will learn why it is the most famous r.v. of all, and why normal approximation usually holds even if we have limited knowledge on the underlying distribution. For $X\\sim N(\\mu, \\sigma^2)$, it takes values over the entire real number line, from negative to positive infinity, or $x \\in \\Re$. Although its bell-shaped pdf is aesthetically pleasing, its mathematical formula is not as memorable: \n",
    "$$f_X(x)=\\frac{1}{\\sqrt{2\\pi \\sigma}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "It is a two-parameter r.v. with $\\mu$ and $\\sigma^2$ if you have not already realised. \n",
    "\n",
    "#### 2.3 Some notations\n",
    "Most textbooks use $f()$ to denote a pmf/pdf, and capital letters for r.v.. Some may specify the r.v. of interest through the subscript, e.g. $f_X()$. The use of subscripts is extremely helpful when multiple r.v. are involved, say, when both $X$ and $Y$ appear in a system with their associated $f_X(x)$ and $f_Y(y)$. The lowercase $x$ inside the round brackets indicates the value at which the pmf/pdf is evaluated. These small $x$ or $y$ are real numbers (not r.v.). Numbers are numbers, r.v. are r.v.. \n",
    "\n",
    "Some texts may even state the associated parameter(s) $\\theta$ in the pmf/pdf, say, $f(x; \\theta)$ or $f(x|\\theta)$. The pipe $|$ reads as \"$x$ given $\\theta$\". \n",
    "\n",
    "#### 2.4 Built-in statistical tables in R\n",
    "We should always make good use of the built-in statistical tables in R. For example, <code>pnorm()</code>, <code>dnorm</code>, <code>qnorm()</code>, and <code>rnorm()</code> for the normal distribution. The prefix <code>p</code> returns the cmf/cdf (see #3.2 below), <code>d</code> for pmf/pdf, <code>q</code> for quantiles (e.g. for hypothesis testing), and <code>r</code> for random number generation. We will try these functions out in today's practical. \n",
    "\n",
    "### 3. Probability and cumulative functions\n",
    "#### 3.1 Properties of probability mass and density functions\n",
    "Per discussed, pmf/pdf are functions to describe the relative probabilities of the outcomes and to characterise a r.v.. From the first axiom, probabilities are non-negative, hence the pmf/pdf never go below the horizontal axis. From the second axiom, we learnt that the sum of pmf bars must be one:\n",
    "$$\\sum_{all~possible~outcomes} f_X(x)=1$$\n",
    "For continuous case, if we take the limit of summation (of vertical bars) it becomes integration: \n",
    "$$\\int_{all~possible~outcomes} f_X(x)dx=1$$\n",
    "That is, the *area* under a pdf must be one. \n",
    "\n",
    "#### 3.2 Cumulative mass and density functions\n",
    "We use capital $F()$ to denote cumulative functions (cmf/cdf). As its name suggests, $F_X(x)=Pr(X\\leqslant x)$m, hence cumulative. It is a monotonically-increasing function with $F(-\\infty)=0$ and $F(\\infty)=1$. For discrete r.v., \n",
    "$$F_X(x)=\\sum_{x_i\\leqslant x}f_X(x_i)$$\n",
    "For continuous case, $F_X(x)$ is the area under the pdf curve, from $-\\infty$ to $x$: \n",
    "$$F_X(x)=\\int_{-\\infty}^{x}f_X(t)dt$$\n",
    "You should know that $t$ is a dummy variable as shown in the fundamental theorem of calculus. Conversely, the pdf is the first derivative of the cdf. You only need either the cumulative or probability function to characterise a r.v. \n",
    "\n",
    "### 4. Statistical moments and expectation\n",
    "#### 4.1 Expectation\n",
    "Imagine an experiment can be repeated for *infinitely* many times. Imagine you keep tossing a coin or keep drawing random numbers from a given distribution for *infinitely* many times. The expecation is the \"average\" of the said experiment. \n",
    "\n",
    "Of course this \"average\" is a hypthetical one as nobody can afford having *infinitely* many repeats. Here we describe the \"average\" behaviour of a r.v. on the population level. Try not to confuse with the \"sample average\" that we tend to calculate from real data (of finite observations). In fact, there is no data in today's discussion. We are merely discussing the characteristics of r.v. based on some given random mechanisms. \n",
    "\n",
    "For discrete r.v., \n",
    "$$E[X]=\\sum_{all~possible~outcomes}xf(x)$$\n",
    "For continuous r.v., \n",
    "$$E[X]=\\int_{-\\infty}^{+\\infty}xf(x)dx$$\n",
    "You can replace the bounds of the integral by the support of $X$. $E[X]$ is the expected value of $X$, the \"average\" value weighted according to the pmf/pdf. $E[X]$ is also called the population mean or true mean of the r.v. $X$. It is a measure of central tendency. \n",
    "\n",
    "#### 4.2 Variance\n",
    "Similarly, we have the population variance, which is given by\n",
    "$$Var[X]=E[(X-E[X])^2]$$\n",
    "The formula above suggests that variance is the expected distance squared of the r.v. $X$ from its population mean. In practice we tend to use this alternative form: \n",
    "$$Var[X]=E[X^2]-(E[X])^2$$\n",
    "There is no surprise that variance is a measure of dispersion. \n",
    "\n",
    "#### 4.3 Higher moments\n",
    "In general, the $n^{th}$ *raw* moment of $X$ is $E[X^n]$: \n",
    "$$E[X^n]=\\int x^nf(x)dx$$\n",
    "\n",
    "And the $n^{th}$ central moment is $E[(X-E[X])^n]$. In most cases only the first few moments are studied. For example, the third moment of a r.v. describes its skewness (e.g. a normal r.v. has 0 third central moment as a bell curve is symmetric about $\\mu$), and the fourth moments is a measure of kurtosis (fat tails). \n",
    "\n",
    "Note that not all distributions have finite moments. One example is the Cauchy distribution (t-distribution with 1 degree of freedom) whose $E[X]$ is undefined. \n",
    "\n",
    "#### 4.4 More on the expectation operator\n",
    "After taking the expectation from a r.v., we get a real number. Note that expectation is linear: \n",
    "$$E[aX+bY]=aE[X]+bE[Y]$$\n",
    "for any r.v. $X$, $Y$ and any real numbers $a$, $b$. \n",
    "\n",
    "In some cases, we may be required to transform a r.v. or to calculate the expectation of a transformed r.v.: \n",
    "$$E[g(X)]=\\int g(x)f(x)dx$$ for any real function $g$. \n",
    "\n",
    "Note that $g(X)$ itself is another r.v. with its own support, pdf/pmf, expecation, etc.. The same is true for $(X+Y)$, that is, the sum (or prodictof r.v. is another r.v.. Remember, transformation of a r.v. yields another r.v.. A r.v. will not suddenly turn into a real number. \n",
    "\n",
    "#### 4.5 Moment generating function\n",
    "A moment generating function (mgf) is the third way to characterise a r.v. (alongside the probability and cumulative functions). $M_X(t)$ is a carefully crafted function from $X$ such that it \"generates\" statistical moments through its derivatives at $t=0$, note that $t$ is a dummy variable. The $n^{th}$ moment of $X$ is: \n",
    "$$E[X^n]=\\frac{d^nM_X(t)}{dt^n}|_{t=0}$$\n",
    "\n",
    "It is a Laplace transform, $M_X(t)=E[e^{tX}]$, if you are wondering. \n",
    "\n",
    "### 5. Central limit theorem and Weak law of large numbers\n",
    "#### 5.1 Central limit theorem\n",
    "Let $\\{X_1, X_2, X_3, ..., X_n\\}$ be i.i.d. r.v. with finite $E[X_i]=\\mu$ and finite $Var[X_i]=\\sigma^2$. Also let $\\bar{X_n}=(X_1+X_2+...+X_n)/n$ be the sample mean of these r.v. ($\\bar{X_n}$ is another r.v.). The central limit theorem states that as $n\\rightarrow \\infty$, the r.v. $\\sqrt{n}(\\bar{X_n}-\\mu)$ converges *in distribution* to a normal distribution: \n",
    "$$\\sqrt{n}(\\bar{X_n}-\\mu) \\xrightarrow{d} N(0,\\sigma^2)$$\n",
    "\n",
    "See today's practical for visualisation. \n",
    "\n",
    "#### 5.2 Weak law of large numbers\n",
    "Let us consider a similar series of i.i.d. r.v. $\\{X_1, X_2, X_3, ..., X_n\\}$ with finite $E[X_i]=\\mu$. The weak law of large numbers states that the sample mean $\\bar{X_n}$ converges *in probability* to the expected value when $n\\rightarrow \\infty$: \n",
    "$$\\bar{X_n}\\xrightarrow{P} \\mu$$\n",
    "as $N$ goes large. Note that converge in probability means the r.v. (on left hand side of the arrow) converges to and lies at the neighbourhood of the fixed number $\\mu$. Some may say, for any postive $\\epsilon$, \n",
    "$$\\lim_{n\\rightarrow \\infty}Pr(|\\bar{X_n}-\\mu|<\\epsilon)=1$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
