{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "274fb19d-7dc3-417e-aefc-58ee3c23007d",
   "metadata": {},
   "source": [
    "## MLE Notebook 3 Theoretical guarantees\n",
    "### 0. Contents\n",
    "More MLE examples: linear regression, general linearised regression (logistic and Poisson). \n",
    "\n",
    "Using <code>optim()</code> to optimise multivariate functions. \n",
    "\n",
    "Properties of ML estimators: asymptotically unbiased, asymptotically efficient, consistent, asymptotically normal, invariance. \n",
    "\n",
    "### 1. More MLE examples\n",
    "#### 1.1 Linear regression\n",
    "I hope we are all familiar with the simplest form of a linear regression. The data comprises a vector of response $\\underline{y}$ and independent (or explanatory) variable $\\underline{x}$: \n",
    "$$y_i=a+bx_i+\\epsilon_i$$\n",
    "where $a$ is the intercept and $b$ is the slope. $i=1, 2, ..., n$ ($n$ is the sample size). $\\epsilon_i$ is the error term, and in simple linear regression, $\\epsilon_i$ are assumed to be i.i.d. $N(0,\\sigma^2)$. \n",
    "\n",
    "Now we have identified the triplet required for MLE: 1) our data $\\underline{x}$ and $\\underline{y}$, 2) the three parameters of interest: $\\{a, b, \\sigma^2\\}$, 3) and finally the model, which is i.i.d. normal errors. If we rearrange the regression equation such that $\\epsilon_i$ is the subject:\n",
    "$$\\epsilon_i=y_i-a-bx_i$$\n",
    "Then the likelihood becomes\n",
    "$$L(\\underline{\\theta})=f(\\epsilon_1\\epsilon_2...\\epsilon_n)=\\prod_{i=1}^nf(\\epsilon_i)$$\n",
    "The first $f$ is the (higher-dimensional) joint pdf of all the error terms. Because the model assumes independent errors, it can be expressed as the product of the univariate pdfs. Now the problem has reverted back to the one we saw in #4.6 yesterday, that we need to find a set of $(\\hat{a}, \\hat{b}, \\hat{\\sigma^2})$ that maximises $L(\\underline{\\theta})$. I definitely would delegate this task to R, but in principle we can find the three partial derivatives and solve the system of three equations. See today's practical on <code>recapture.csv</code>, with solutions given in a separate notebook. In R, we can use the built-in <code>dnorm(y-a-b*x, mean=0, sd=sigma)</code> to get the normal pdf directly. \n",
    "\n",
    "Note: In linear regressions with normal errors, maximising log-likelihood is equivalent to minimising sum of squares (least squares). \n",
    "\n",
    "#### 1.2 Logistic regression\n",
    "Logistic regression belongs to the family of generalised linear models (GLM). It aims to relate a binary response to a set of explanatory variable(s). For example, in public health a typical binary response will be the state of the subjects (dead/alive, absent/present, ...). When I think of binary, I immediately recall the Bernoulli r.v., whose outcome is either 0 or 1. In fact, logistic regression assumes each response follows a Bernoulli distribution with its own $p_i$:\n",
    "$$Y_i\\sim Bernoulli(p_i)$$\n",
    "$i=1, 2, ..., n$. We also understand that $p_i$ is affected by our explanatory variable $x_i$:\n",
    "$$p_i=\\eta^{-1}(a+bx_i)$$\n",
    "$a$ and $b$ are the coefficients, similar to the intercept and slope in linear regression. $(a+bx_i)$ is called the linear predictor. The remaining question is, why do we need $\\eta^{-1}$? The parameter $p_i$ from Bernoulli has to be bounded between 0 and 1, but $(a+bx_i)$ can be any real number. That is why we call upon $\\eta^{-1}$ to map a real number to $[0, 1]$: \n",
    "$$\\eta^{-1}(a+bx_i)=\\frac{e^{a+bx_i}}{1+e^{a+bx_i}}$$\n",
    "In logistic regression $\\eta^{-1}$ is the *expit* transformation, whereas its inverse $\\eta$ is *logit*, hence the name of the model. \n",
    "\n",
    "With the triplet identified we can proceed to find the ML estimates for $a$ and $b$. See the worked example on <code>flowering.txt</code> in today's practical (also available as a separate notebook). \n",
    "\n",
    "#### 1.3 Poisson regression\n",
    "Poisson regression is GLM to model count response (e.g. number of mutations/crossovers, malaria cases, mosquito count). As its name suggests, it assumes each response follows a Poisson distribution with its own rate parameter $\\lambda_i$:\n",
    "$$Y_i\\sim Poisson(\\lambda_i)$$\n",
    "$i=1, 2, ..., n$. Similar to the logistic case, we need a function to link up $\\lambda_i$ with the linear predictor. The constraint here is that the rate $\\lambda_i$ must be non-negative. An appropriate transformation is exponential: \n",
    "$$\\lambda_i=e^{a+bx_i}$$\n",
    "Note that the inverse of exponential is log. \n",
    "\n",
    "In GLM, a link function provides the relationship between the lienar predictor and the mean of the distribution function. Logit and log are link functions in logistic and Poisson GLMs, respectively. I hope these examples help demystify GLMs and their model fitting via <code>glm(y~x, family=)</code>. \n",
    "\n",
    "#### 1.4 quasi-family and overdispersion\n",
    "For binomial and Poisson GLM, overdispersion occurs when the data exhibits greater variability than the model specified. One option is to fit a <code>family=quasibinomial</code> or <code>family=quasipoisson</code> from <code>glm()</code>. Note that quasi is a post-hoc method to adjust for p-values. \n",
    "\n",
    "For count data, you may consider using a negative-binomial GLM via <code>glm.nb()</code>. It is a two-parameter model which allows the estimation of the dispersion parameter under MLE. Another way is to introduce observation-level random effect (Harrison, 2014), but the model is now a mixed-effect GLM (GLMM?). Fit via <code>glmer()</code> from <code>library(lme4)</code>. Works for both proportional and count response. \n",
    "\n",
    "### 2. <code>optim()</code>\n",
    "<code>optim()</code> is a generic optimisation routines for multivariate functions. Here multivariate refers to the number of parameters rather than observations. It is so powerful that its help file <code>?optim</code> provides more questions than answers. \n",
    "#### 2.1 Inputs\n",
    "To run <code>optim()</code> properly we need several input arguments, usually given in the following order:\n",
    "1) The mandatory <code>par=</code> vector gives the initial condition for the search. If it is a $k$-dimensional optimisation then a vector of length $k$ should be supplied. Avoid starting near the boundary of the parameter space. \n",
    "2) Put the name of the function you wish to optimise under <code>fn=</code>. Mandatory.\n",
    "3) <code>method=</code> specifies the optimisation algorithm to be used. The default is <code>Nelder-Mead</code>. The choice of algorithm can affect performance and perhaps numerical results. Another popular option is <code>method='L-BFGS-B'</code> which takes a box-like constraint (see below).\n",
    "4) The default algorithm imposes no constraints on the parameter space which is a double-edged sword. When <code>method='L-BFGS-B'</code> is chosen then you must supply <code>lower=</code> and <code>upper=</code> bounds as two vectors (with lengths equal the number of parameters). This algorithm is useful when some parameters cannot go beyond a certain value (e.g. $\\sigma>0$ in a linear regression). \n",
    "5) <code>control=list((fnscale=-1))</code> means to maximise the function. The default is minimise. Other control options, such as tolerance and number of iterations, can be added to this list. \n",
    "6) <code>hessian=T</code> to return the Hessian matrix. See tomorrow's notebook. \n",
    "\n",
    "#### 2.2 Outputs\n",
    "If success it returns a big list: \n",
    "1) <code>\\$par</code> is the parameter values that maximise the function. \n",
    "2) <code>\\$value</code> returns the maximised function value (maximised log-likelihood). \n",
    "3) The rest are for performance and convergence checking. If you have asked for the Hessian matrix you may find it in the output as well. \n",
    "\n",
    "R has other optimisation routines such as <code>nlm()</code> and <code>nlminb</code>, I think they serve the same purpose. External packages are available as well. \n",
    "\n",
    "### 3. Properties of ML estimators\n",
    "Below are the theoretical guarantees of ML estimators. They also justify why we are spending a whole week on likelihood. \n",
    "\n",
    "#### 3.1 ML estimators are asymptotically unbiased\n",
    "If we repeat the experient (and ML estimation) for infinitely many times and obtain many $\\hat{\\theta}$, then the hypothetical average of these $\\hat{\\theta}$ is $\\theta$. When we talk of a hypothetical average we mean expectation: \n",
    "$$E[\\hat{\\theta}]\\rightarrow \\theta$$\n",
    "when sample size is large, $n\\rightarrow \\infty$. \n",
    "\n",
    "#### 3.2 ML estimators are asymptotically efficient\n",
    "Efficiency means ML estimator $\\hat{\\theta}$ usually has a lower variance compared to the other estimators (what are the other estimators?). Since it has better use of the data, it produces narrower confidence intervals (C.I.). In particular, $Var[\\hat{\\theta}]$ appraches the theoretical lower bound (Cram√©r-Rao lower bound) as $n\\rightarrow \\infty$. \n",
    "\n",
    "#### 3.3 ML estimators are consistent\n",
    "The ML estimator $\\hat{\\theta}$ converges *in probability* to the true parameter $\\theta$ when $n\\rightarrow \\infty$. \n",
    "\n",
    "#### 3.4 ML estimators are asymptotically normal\n",
    "The ML estimator $\\hat{\\theta}$ is asymptotically distributed as normal with mean equals the true paramter $\\theta$. This is closely related to the central limit theorem, and explains why the magic number 1.96 works in constructing C.I. and z-tests. More on C.I. construction tomorrow. \n",
    "\n",
    "#### 3.5 The invariant principle of ML stimators\n",
    "If $\\hat{\\theta}$ is the ML estimator for the parameter $\\theta$, then $g(\\hat{\\theta})$ is the ML estimator for $g(\\theta)$. That is, we do not need to recalculate the ML estimator if the transformed parameter is of concern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67fb7c3-95a0-44da-8415-03bc279d21f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R-4.3.1",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
