{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53a6fb4a-75af-4d06-bc36-d8a1af30d314",
   "metadata": {},
   "source": [
    "## Day 4 Hypothesis testing, Interval estimation\n",
    "\n",
    "### 0. Contents\n",
    "Hypothesis testing: Likelihood-Ratio test\n",
    "\n",
    "Interval estimation: likelihood-based inference on one paramter. Joint confidence region for two (or more) paramters via Profile likelihood. Wald C.I. with normal approximation. \n",
    "\n",
    "### 1. Statistical inference\n",
    "Statistical inference often refers to the procedures including point estimation, interval estimation, and hypothesis testing. For point estimation, it simply refers to finding the \"best guess\" for a parameter. The point estimate is usually accompanied by a confidence interval (C.I.) to reflect the uncertainty surrounding the \"best guess\". Hypothesis testing is heavily involved in model selection and simplification, or testing for the difference between treatment groups in a trial. Luckily, likelihood has all these covered. \n",
    "\n",
    "### 2. Likelihood-Ratio test\n",
    "Likelihood-Ratio test (LRT) is a likelihood-based hypothesis testing procedure. It applies to two *nested* models aiming to decide which one we should adopt. Let M1 and M2 be two models, and that M1 is nested in M2 (i.e. M1 is a special case of M2). We also assume $d1$ and $d2$ are the number of free parameters for M1 and M2 respectively, with $d2>d1$. The LRT statistic $D$ is twice the difference of their maximised log-likelihoods, and $D$ approximately follows a $\\chi^2$ distribution with $(d2-d1)$ degrees of freedom. \n",
    "\n",
    "The procedure of performing a LRT is as follows: \n",
    "1) Fit M1 to the data, record the maximised log-likelihood value, let us call it $\\ln(L1)$. The parameter estimates are unimportant here. \n",
    "2) Fit M2 to the same data, separately, record the maximised log-likelihood value $\\ln(L2)$. Note that $\\ln(L2)\\geqslant\\ln(L1)$. \n",
    "3) Compute the LRT statitsic $D=2*[\\ln(L2)-\\ln(L1)]$. \n",
    "4) Look up $\\chi^2_{d2-d1}$ table for a critical value. Accept M1 as the simplidfied model if $D$ is smaller than the critical value.\n",
    "\n",
    "The intuition of $D$ is to measure the difference in explanatory power between the two models. M2, being the full model, must fit the data better with a larger maximised log-likelihood. M1 has fewer parameters hence poorer fit. If the parameters dropped by M1 are unimportant, then there should be little difference between the two models, hence small $D$. If $D$ is smaller than the critical value then we accept M1 as the simplified model (principle of parsimony). \n",
    "\n",
    "In today's practical we will apply LRT to test for the signficance of the intercept term in a linear regression. In this case, the full model M2 is the three-parameter model, whereas M1 is the simplified model with $a=0$. We will also revisit the coin tossing example to test for (or against) the fair-coin hypothesis. \n",
    "\n",
    "### 3. Interval estimation\n",
    "#### 3.1 Likelihood-based C.I. (one-parameter case)\n",
    "LRT accepts the simpler model if $D$ is smaller than half of the critical value of $\\chi^2_1$, assuming M1 and M2 differ by one parameter. At $\\alpha=0.05$, the critical value is 3.84, as obtained from <code>qchisq(0.95, df=1)</code>\n",
    "\n",
    "With this argument, we can perform a series of LRTs against many M1 with different $\\tilde{\\theta}$ values. As we move $\\tilde{\\theta}$ away from $\\hat{\\theta}$, the $D$ statistic will get larger and eventually hit the decision boundary to reject M1. As a result, the 95% C.I. for $\\theta$ in the univariate case is the collection of $\\tilde{\\theta}$ values whose M1 remain accepted by LRT. Equivalently, the collection of $\\tilde{\\theta}$ whose log-likelihood is within 3.84/2=1.92 (or 2) units from the peak of the log-likelihood curve $l(\\hat{\\theta})$. \n",
    "\n",
    "Example: In a coin tossing experiment we obtained 7 heads and 3 tails. On day 2 we visualised the log-likelihood function. We also learnt that the maximised log-likelihood is $l(\\hat{p})=l(0.7)$. If we draw a horizontal line 2 units below the peak: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "649f8f9c-8e39-4ec4-80ae-10a625ad2e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEX9/v0AAAAAAP9MTUxn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\nEgQICQKEBAFCggAhQcD/A3xqKe5dnuszAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# THE COIN TOSSING LOG-LIKELIHOOD FUNCTION\n",
    "log.binomial.likelihood<-function(p) {lchoose(10,7)+7*log(p)+3*log(1-p)}\n",
    "p<-seq(0, 1, 0.01)\n",
    "log.likelihood.values<-log.binomial.likelihood(p)\n",
    "plot(p, log.likelihood.values, ylab='log-likelihood', type='l', lwd=2)\n",
    "# DRAW A LINE 2 UNITS BELOW THE PEAK\n",
    "abline(h=log.binomial.likelihood(0.7)-2, col='red', lty=2, lwd=2)\n",
    "# THE 95% C.I.\n",
    "abline(v=c(0.39, 0.92), col='blue', lty=3, lwd=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba522ab-3ab9-4c9e-bbde-6189a43225da",
   "metadata": {},
   "source": [
    "The intersections between the log-likelihood and the minus-two line are the lower and upper 95% C.I. for $p$, [0.39, 0.92] in this case. Use <code>uniroot()</code> to find the two intersection points if you want to be ultra precise. \n",
    "\n",
    "#### 3.2 C.I. and joint confidence region in a two-parameter model\n",
    "Combining individual C.I.s for multiple parameters can be problematic as the overall $\\alpha$ level cannot be maintained. Corrections (e.g. Bonferroni) can be applied to individual tests to preserve the overall $\\alpha$, but the details are not discussed here. Another concern is the correlation among ML estimators. \n",
    "\n",
    "There are proper ways to find the *joint confidence region* for two or more parameters. The log-likelihood surface for a two-parameter model can be visualised by a 3D plot (via <code>persp()</code>) or a <code>contour()</code> plot. The 95% C.I. for *one* parameter alone can be found using the same -1.92 rule as previously desbribed. For the 95% joint confidence region for *both* parameters, we look for the collection of parameter pairs whose log-likelihood are within $0.5*\\chi^2_{2, 0.95}=2.99$ units from the maximum. See #4 of <code>recapture.csv</code> notebook for illustration. \n",
    "\n",
    "#### 3.3 Profile likelihood \n",
    "Profiling is a generalisation of #3.2. Let us partition the vector of parameters into two subsets: $\\underline{\\theta}=(\\underline{\\theta_1},\\underline{\\theta_2})$. The motivation is to find the C.I. (or region) for $\\underline{\\theta_1}$. The procedure involves *partial* maximisation of the original log-likelihood along $\\underline{\\theta_1}$. If we fix $\\underline{\\theta_1}=\\underline{\\tilde{\\theta_1}}$, then we can vary $\\underline{\\theta_2}$ such that the log-likelihood is partially maximised. This gives the profile log-likelihood value at $\\underline{\\tilde{\\theta_1}}$ (we can call it $l^*(\\underline{\\tilde{\\theta_1}})$ with an asterisk). And if we repeat the same for many other $\\underline{\\tilde{\\theta_1}}$, then we obtain a profile log-likelihood function for the parameter subset $\\underline{\\theta_1}$. \n",
    "\n",
    "Mathematically,$$l^*(\\underline{\\theta_1})=\\max_{\\underline{\\theta_2}} l(\\underline{\\theta_1}, \\underline{\\theta_1}; \\underline{x})$$\n",
    "\n",
    "See today's practical on <code>flowering.txt</code>. In general, the 95% joint confidence region for $k$ parameters is the collection of parameter values for which the log-likelihood decreases by no more than half of $\\chi^2_{0.95~df=k}$ from its maximum. \n",
    "\n",
    "#### 3.4 Confidence interval via normal approximation (Wald C.I.)\n",
    "##### 3.4.1 Univartiate Wald C.I.\n",
    "One key property of ML estimators is approximate normality under reasonably large sample size. It means that the 95% C.I. for the true parameter $\\theta$ is approximately $\\hat{\\theta}\\pm1.96\\sqrt{Var(\\hat{\\theta})}$. The magic number 1.96 comes from the 2.5- and 97.5-percentile of the standard normal distribution. The remaining question is to find $Var(\\hat{\\theta})$. Wald C.I.s (those inferred from approximate normality) and tests are extensively used <code>lm()</code> and <code>glm()</code> to test for effect significance. Have you wondered how the standard errors from a <code>summary()</code> table are computed? In fact, they can be approximated from the *curvature* of the log-likelihood:\n",
    "$$Var(\\hat{\\theta})\\approx-\\frac{1}{l''(\\hat{\\theta})}$$\n",
    "where $l''(\\hat{\\theta})$ is the second derivative of the log-likelihood function evaluated at $\\hat{\\theta}$. Note that the second derivative is always negative given mle is a global maximum therefore a -1 is required in the numerator to ensure $Var(\\hat{\\theta})>0$. If the log-likelihood is steeper (i.e. descends faster) around its peak then the C.I. should be narrower, thus smaller $Var(\\hat{\\theta})$. Therefore \"steepness\" and variance are inversely proportional as reflected in the formula. \n",
    "\n",
    "##### 3.4.2 Multivariate case\n",
    "The principle still holds for multivariate (multiple-parameter) case as the ML estimators follow a multivariate normal when $n\\rightarrow\\infty$. The \"variance\" is no longer a number but becomes a $k$-by-$k$ variance-covariance matrix $\\boldsymbol{V}(\\underline{\\hat{\\theta}})$ for $k$ parameters. Similar to the univariate case, it is related to the second derivative of the log-likelihood surface. Empirically, \n",
    "$$\\boldsymbol{V}(\\underline{\\hat{\\theta}})\\approx-\\boldsymbol{H}(\\underline{\\hat{\\theta}})^{-1}$$\n",
    "where $\\boldsymbol{H}(\\underline{\\hat{\\theta}})$ is the $k$-by-$k$ Hessian matrix, the second derivative of the $l(\\underline{\\theta})$ evaluated at $\\underline{\\hat{\\theta}}$. It sounds familiar as we can request <code>hessian=T</code> from <code>optim()</code>. $-\\boldsymbol{H}(\\underline{\\hat{\\theta}})$ is also called the observed Fisher information matrix, which tells the amount of information contained towards the parameters. \n",
    "\n",
    "The final step is to calculate the negative inverse of the Hessian matrix, which can be completed via the <code>solve()</code> function (note: we use <code>solve()</code> to solve linear equations). With $\\boldsymbol{V}(\\underline{\\hat{\\theta}})$ we can infer the Wald C.I. for a single parameter under the usual way, or perform multivariate testing for a subset of parameters (beyond the scope of this module). \n",
    "\n",
    "“In Author’s experience, the Wald (i.e. normality) and likelihood method can give quite different results when used to test joint hypotheses… The likelihood method can requie more effort to compute, but is generally preferred.” (Millar 2011)\n",
    "\n",
    "##### 3.5 Interpretations of C.I.\n",
    "Classical statistics considers parameter $\\theta$ as a fixed but unknown number. If the experiment is repeated for infinitely many times and that infinitely many $\\hat{\\theta}$ and 95% C.I.s are obtained, then 95% of these C.I.s will cover the true parameter $\\theta$. \n",
    "\n",
    "This concept is completely revolutionised by the Bayesian school, that it (weirdly, perhaps) believes $\\theta$ can also treated as a r.v. thus has its own distributions (e.g. prior and posterior distributions). Therefore, making probabilistic statements towards $\\theta$ is permitted (i.e. there is 95% chance that $\\theta$ is between such and such value). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ce27f-0abd-46b6-a36d-ab849869a864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R-4.3.1",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
